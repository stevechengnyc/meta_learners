{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "890f27a2",
      "metadata": {
        "id": "890f27a2"
      },
      "source": [
        "# Uplift Modeling with EconML using MovieLens 1M\n",
        "This notebook downloads MovieLens 1M data, simulates treatment and renewal outcomes, introduces missing data, imputes values, and trains S-, T-, and X-Learners using different base learners."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "93f94df8",
      "metadata": {
        "id": "93f94df8",
        "outputId": "1d432015-7049-44c2-c2d5-309eb72d8c79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting econml\n",
            "  Downloading econml-0.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (38 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>1.4.0 in /usr/local/lib/python3.11/dist-packages (from econml) (1.15.3)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting sparse (from econml)\n",
            "  Downloading sparse-0.16.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from econml) (1.5.0)\n",
            "Requirement already satisfied: statsmodels>=0.10 in /usr/local/lib/python3.11/dist-packages (from econml) (0.14.4)\n",
            "Collecting shap<0.44.0,>=0.38.1 (from econml)\n",
            "  Downloading shap-0.43.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (from econml) (4.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from econml) (24.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap<0.44.0,>=0.38.1->econml) (4.67.1)\n",
            "Collecting slicer==0.0.7 (from shap<0.44.0,>=0.38.1->econml)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap<0.44.0,>=0.38.1->econml) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap<0.44.0,>=0.38.1->econml) (3.1.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.10->econml) (1.0.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap<0.44.0,>=0.38.1->econml) (0.43.0)\n",
            "Downloading econml-0.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shap-0.43.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (532 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Downloading sparse-0.16.0-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.3/147.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: slicer, numpy, sparse, scikit-learn, shap, econml\n",
            "  Attempting uninstall: slicer\n",
            "    Found existing installation: slicer 0.0.8\n",
            "    Uninstalling slicer-0.0.8:\n",
            "      Successfully uninstalled slicer-0.0.8\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: shap\n",
            "    Found existing installation: shap 0.47.2\n",
            "    Uninstalling shap-0.47.2:\n",
            "      Successfully uninstalled shap-0.47.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed econml-0.15.1 numpy-1.26.4 scikit-learn-1.5.2 shap-0.43.0 slicer-0.0.7 sparse-0.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install econml scikit-learn pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bb1bc30e",
      "metadata": {
        "id": "bb1bc30e",
        "outputId": "9d75e681-cf6b-43b3-b397-d7858fe9876f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-17 14:02:15--  https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5917549 (5.6M) [application/zip]\n",
            "Saving to: ‘ml-1m.zip.1’\n",
            "\n",
            "ml-1m.zip.1         100%[===================>]   5.64M  8.64MB/s    in 0.7s    \n",
            "\n",
            "2025-05-17 14:02:16 (8.64 MB/s) - ‘ml-1m.zip.1’ saved [5917549/5917549]\n",
            "\n",
            "Archive:  ml-1m.zip\n",
            "  inflating: ml-1m/ml-1m/movies.dat  \n",
            "  inflating: ml-1m/ml-1m/ratings.dat  \n",
            "  inflating: ml-1m/ml-1m/README      \n",
            "  inflating: ml-1m/ml-1m/users.dat   \n"
          ]
        }
      ],
      "source": [
        "# Download and extract MovieLens 1M dataset\n",
        "!wget https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
        "!unzip -o ml-1m.zip -d ml-1m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e1e232db",
      "metadata": {
        "id": "e1e232db"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from econml.metalearners import SLearner, TLearner, XLearner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "848b323c",
      "metadata": {
        "id": "848b323c",
        "outputId": "1dca9deb-9a6f-4d98-da12-ce7e785f2509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'ml-1m/ratings.dat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9488fe8edcc2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m ratings = pd.read_csv('ml-1m/ratings.dat', sep='::', engine='python',\n\u001b[0m\u001b[1;32m      3\u001b[0m                       names=['UserID', 'MovieID', 'Rating', 'Timestamp'])\n\u001b[1;32m      4\u001b[0m users = pd.read_csv('ml-1m/users.dat', sep='::', engine='python',\n\u001b[1;32m      5\u001b[0m                     names=['UserID', 'Gender', 'Age', 'Occupation', 'Zip-code'])\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ml-1m/ratings.dat'"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "ratings = pd.read_csv('ml-1m/ratings.dat', sep='::', engine='python',\n",
        "                      names=['UserID', 'MovieID', 'Rating', 'Timestamp'])\n",
        "users = pd.read_csv('ml-1m/users.dat', sep='::', engine='python',\n",
        "                    names=['UserID', 'Gender', 'Age', 'Occupation', 'Zip-code'])\n",
        "movies = pd.read_csv('ml-1m/movies.dat', sep='::', engine='python',\n",
        "                     names=['MovieID', 'Title', 'Genres'])\n",
        "df = ratings.merge(users, on='UserID').merge(movies, on='MovieID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52cba61f",
      "metadata": {
        "id": "52cba61f"
      },
      "outputs": [],
      "source": [
        "# Feature creation\n",
        "np.random.seed(42)\n",
        "df['WatchTime'] = df['Rating'] * np.random.uniform(15, 30, size=len(df))\n",
        "df['TenureMonths'] = (df['Timestamp'] - df['Timestamp'].min()) // (60*60*24*30)\n",
        "user_features = df.groupby('UserID').agg({\n",
        "    'WatchTime': 'sum',\n",
        "    'MovieID': 'nunique',\n",
        "    'TenureMonths': 'max',\n",
        "    'Age': 'first',\n",
        "    'Occupation': 'first'\n",
        "}).rename(columns={'WatchTime': 'TotalWatchTime', 'MovieID': 'UniqueMovies'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6bbdadf",
      "metadata": {
        "id": "a6bbdadf"
      },
      "outputs": [],
      "source": [
        "# Introduce and impute missing data\n",
        "user_features.loc[user_features.sample(frac=0.1).index, 'TotalWatchTime'] = np.nan\n",
        "user_features.loc[user_features.sample(frac=0.1).index, 'TenureMonths'] = np.nan\n",
        "user_features['TotalWatchTime'].fillna(user_features['TotalWatchTime'].median(), inplace=True)\n",
        "user_features['TenureMonths'].fillna(user_features['TenureMonths'].median(), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9256a93",
      "metadata": {
        "id": "b9256a93"
      },
      "outputs": [],
      "source": [
        "# Simulate treatment and renewal\n",
        "user_features['treatment'] = np.random.binomial(1, 0.5, size=len(user_features))\n",
        "engaged = user_features['TotalWatchTime'] > user_features['TotalWatchTime'].median()\n",
        "base_rate = 0.2\n",
        "uplift = 0.15 * ((user_features['treatment'] == 1) & engaged).astype(float)\n",
        "user_features['renewed'] = np.random.binomial(1, base_rate + uplift)\n",
        "X = user_features[['TenureMonths', 'TotalWatchTime', 'UniqueMovies']]\n",
        "T = user_features['treatment'].values\n",
        "Y = user_features['renewed'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06b3fb79",
      "metadata": {
        "id": "06b3fb79"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, T_train, T_test, Y_train, Y_test = train_test_split(X, T, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d90cd1d",
      "metadata": {
        "id": "0d90cd1d"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning\n",
        "lr_grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid={'C': [0.01, 0.1, 1, 10]}, cv=3)\n",
        "lr_grid.fit(X_train, Y_train)\n",
        "best_lr = lr_grid.best_estimator_\n",
        "\n",
        "rf_random = RandomizedSearchCV(RandomForestRegressor(random_state=42),\n",
        "    param_distributions={'n_estimators': [100, 200], 'max_depth': [None, 10, 20]},\n",
        "    n_iter=4, cv=3, random_state=42)\n",
        "rf_random.fit(X_train, Y_train)\n",
        "best_rf = rf_random.best_estimator_\n",
        "\n",
        "gb_grid = GridSearchCV(GradientBoostingRegressor(random_state=42),\n",
        "    param_grid={'n_estimators': [100, 150], 'learning_rate': [0.05, 0.1]}, cv=3)\n",
        "gb_grid.fit(X_train, Y_train)\n",
        "best_gb = gb_grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04cfa469",
      "metadata": {
        "id": "04cfa469"
      },
      "outputs": [],
      "source": [
        "# Train learners\n",
        "s_learner = SLearner(learner=best_lr)\n",
        "t_learner = TLearner(models=best_rf)\n",
        "x_learner = XLearner(models=best_gb)\n",
        "s_learner.fit(Y_train, T_train, X=X_train)\n",
        "t_learner.fit(Y_train, T_train, X=X_train)\n",
        "x_learner.fit(Y_train, T_train, X=X_train)\n",
        "s_te = s_learner.effect(X_test)\n",
        "t_te = t_learner.effect(X_test)\n",
        "x_te = x_learner.effect(X_test)\n",
        "pd.DataFrame({'S_Learner': s_te, 'T_Learner': t_te, 'X_Learner': x_te}).head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}