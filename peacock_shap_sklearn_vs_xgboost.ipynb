{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ca570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # SHAP Comparison: sklearn vs XGBoost Gradient Boosting on Peacock Renewal Data\n",
    "\n",
    "# üì¶ Install dependencies\n",
    "!pip install shap xgboost scikit-learn pandas matplotlib seaborn\n",
    "\n",
    "# üì• Upload the dataset: peacock_user_data_with_renewed_and_propensity.csv\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# üìä Load data\n",
    "import pandas as pd\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"peacock_user_data_with_renewed_and_propensity.csv\")\n",
    "\n",
    "# Define features and labels\n",
    "X = df.drop(columns=[\"user_id\", \"assigned_promo\", \"renewed\", \"propensity_score\"])\n",
    "y = df[\"renewed\"]\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocessing\n",
    "numeric_features = [\"tenure_months\", \"prior_engagement_score\", \"weekly_watch_hours\", \"num_devices\"]\n",
    "categorical_features = [\"device_type\", \"payment_method\", \"account_type\", \"region\", \"has_kids_profile\", \"promo_eligible\"]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), numeric_features),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# ‚úÖ Train sklearn GradientBoostingClassifier\n",
    "sk_gbm = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=0)\n",
    "sk_gbm.fit(X_train_proc, y_train)\n",
    "\n",
    "# ‚úÖ Train XGBoost\n",
    "xgb_gbm = xgb.XGBClassifier(n_estimators=100, max_depth=3, use_label_encoder=False, eval_metric=\"logloss\", random_state=0)\n",
    "xgb_gbm.fit(X_train_proc, y_train)\n",
    "\n",
    "# üéØ Evaluate both\n",
    "print(\"Sklearn GBM:\")\n",
    "print(classification_report(y_test, sk_gbm.predict(X_test_proc)))\n",
    "print(\"AUC:\", roc_auc_score(y_test, sk_gbm.predict_proba(X_test_proc)[:, 1]))\n",
    "\n",
    "print(\"\\nXGBoost GBM:\")\n",
    "print(classification_report(y_test, xgb_gbm.predict(X_test_proc)))\n",
    "print(\"AUC:\", roc_auc_score(y_test, xgb_gbm.predict_proba(X_test_proc)[:, 1]))\n",
    "\n",
    "# üîç SHAP for sklearn GBM\n",
    "explainer_sk = shap.Explainer(sk_gbm.predict_proba, X_test_proc, feature_names=feature_names)\n",
    "shap_values_sk = explainer_sk(X_test_proc)\n",
    "\n",
    "shap.plots.beeswarm(shap_values_sk, max_display=10, show=False)\n",
    "plt.title(\"SHAP Summary - Sklearn GBM\")\n",
    "plt.show()\n",
    "\n",
    "# üîç SHAP for XGBoost\n",
    "explainer_xgb = shap.Explainer(xgb_gbm, X_test_proc, feature_names=feature_names)\n",
    "shap_values_xgb = explainer_xgb(X_test_proc)\n",
    "\n",
    "shap.plots.beeswarm(shap_values_xgb, max_display=10, show=False)\n",
    "plt.title(\"SHAP Summary - XGBoost\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
